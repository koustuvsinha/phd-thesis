@inproceedings{Clark2020TransformersAS,
  title           = {Transformers as Soft Reasoners over Language},
  author          = {Peter Clark and Oyvind Tafjord and Kyle Richardson},
  booktitle       = {IJCAI},
  year            = 2020
}

@article{Ontan2022LogicInferenceAN,
  title           = {LogicInference: A New Dataset for Teaching Logical
                  Inference to seq2seq Models},
  author          = {Santiago Onta{\~n}{\'o}n and Joshua Ainslie and Vaclav
                  Cvicek and Zachary Kenneth Fisher},
  journal         = {ArXiv},
  year            = 2022,
  volume          = {abs/2203.15099}
}

@article{Zhang2022OnTP,
  title           = {On the Paradox of Learning to Reason from Data},
  author          = {Honghua Zhang and Liunian Harold Li and Tao Meng and
                  Kai-Wei Chang and Guy Van den Broeck},
  journal         = {ArXiv},
  year            = 2022,
  volume          = {abs/2205.11502}
}

@inproceedings{fei-etal-2022-cqg,
  title           = "{CQG}: A Simple and Effective Controlled Generation
                  Framework for Multi-hop Question Generation",
  author          = "Fei, Zichu and Zhang, Qi and Gui, Tao and Liang, Di and
                  Wang, Sirui and Wu, Wei and Huang, Xuanjing",
  booktitle       = "Proceedings of the 60th Annual Meeting of the Association
                  for Computational Linguistics (Volume 1: Long Papers)",
  month           = may,
  year            = 2022,
  address         = "Dublin, Ireland",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2022.acl-long.475",
  doi             = "10.18653/v1/2022.acl-long.475",
  pages           = "6896--6906",
  abstract        = "Multi-hop question generation focuses on generating complex
                  questions that require reasoning over multiple pieces of
                  information of the input passage. Current models with
                  state-of-the-art performance have been able to generate the
                  correct questions corresponding to the answers. However, most
                  models can not ensure the complexity of generated questions,
                  so they may generate shallow questions that can be answered
                  without multi-hop reasoning. To address this challenge, we
                  propose the CQG, which is a simple and effective controlled
                  framework. CQG employs a simple method to generate the
                  multi-hop questions that contain key entities in multi-hop
                  reasoning chains, which ensure the complexity and quality of
                  the questions. In addition, we introduce a novel controlled
                  Transformer-based decoder to guarantee that key entities
                  appear in the questions. Experiment results show that our
                  model greatly improves performance, which also outperforms the
                  state-of-the-art model about 25{\%} by 5 BLEU points on
                  HotpotQA.",
}

@article{gontier2020measuring,
  title           = {Measuring systematic generalization in neural proof
                  generation with transformers},
  author          = {Gontier, Nicolas and Sinha, Koustuv and Reddy, Siva and
                  Pal, Chris},
  journal         = {Advances in Neural Information Processing Systems},
  volume          = 33,
  pages           = {22231--22242},
  year            = 2020
}

@inproceedings{goodwin-etal-2020-probing,
  title           = "Probing Linguistic Systematicity",
  author          = "Goodwin, Emily and Sinha, Koustuv and O{'}Donnell, Timothy
                  J.",
  booktitle       = "Proceedings of the 58th Annual Meeting of the Association
                  for Computational Linguistics",
  month           = jul,
  year            = 2020,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2020.acl-main.177",
  doi             = "10.18653/v1/2020.acl-main.177",
  pages           = "1958--1969",
  abstract        = "Recently, there has been much interest in the question of
                  whether deep natural language understanding (NLU) models
                  exhibit systematicity, generalizing such that units like words
                  make consistent contributions to the meaning of the sentences
                  in which they appear. There is accumulating evidence that
                  neural models do not learn systematically. We examine the
                  notion of systematicity from a linguistic perspective,
                  defining a set of probing tasks and a set of metrics to
                  measure systematic behaviour. We also identify ways in which
                  network architectures can generalize non-systematically, and
                  discuss why such forms of generalization may be unsatisfying.
                  As a case study, we perform a series of experiments in the
                  setting of natural language inference (NLI). We provide
                  evidence that current state-of-the-art NLU systems do not
                  generalize systematically, despite overall high performance.",
}

@inproceedings{minervini2020learning,
  title           = {Learning reasoning strategies in end-to-end differentiable
                  proving},
  author          = {Minervini, Pasquale and Riedel, Sebastian and Stenetorp,
                  Pontus and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  booktitle       = {International Conference on Machine Learning},
  pages           = {6938--6949},
  year            = 2020,
  organization    = {PMLR}
}

@inproceedings{tamari-etal-2022-dyna,
  title           = "{D}yna-b{A}b{I}: unlocking b{A}b{I}{'}s potential with
                  dynamic synthetic benchmarking",
  author          = "Tamari, Ronen and Richardson, Kyle and Kahlon, Noam and
                  Sar-shalom, Aviad and Liu, Nelson F. and Tsarfaty, Reut and
                  Shahaf, Dafna",
  booktitle       = "Proceedings of the 11th Joint Conference on Lexical and
                  Computational Semantics",
  month           = jul,
  year            = 2022,
  address         = "Seattle, Washington",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2022.starsem-1.9",
  pages           = "101--122",
  abstract        = "While neural language models often perform surprisingly
                  well on natural language understanding (NLU) tasks, their
                  strengths and limitations remain poorly understood. Controlled
                  synthetic tasks are thus an increasingly important resource
                  for diagnosing model behavior. In this work we focus on story
                  understanding, a core competency for NLU systems. However, the
                  main synthetic resource for story understanding, the bAbI
                  benchmark, lacks such a systematic mechanism for controllable
                  task generation. We develop Dyna-bAbI, a dynamic framework
                  providing fine-grained control over task generation in bAbI.
                  We demonstrate our ideas by constructing three new tasks
                  requiring compositional generalization, an important
                  evaluation setting absent from the original benchmark. We
                  tested both special-purpose models developed for bAbI as well
                  as state-of-the-art pre-trained methods, and found that while
                  both approaches solve the original tasks (99{{\%} accuracy),
                  neither approach succeeded in the compositional generalization
                  setting, indicating the limitations of the original training
                  data.We explored ways to augment the original data, and found
                  that though diversifying training data was far more useful
                  than simply increasing dataset size, it was still insufficient
                  for driving robust compositional generalization (with 70{{\%}
                  accuracy for complex compositions). Our results underscore the
                  importance of highly controllable task generators for creating
                  robust NLU systems through a virtuous cycle of model and data
                  development.",
}

@inproceedings{tian-etal-2021-diagnosing,
  title           = "Diagnosing the First-Order Logical Reasoning Ability
                  Through {L}ogic{NLI}",
  author          = "Tian, Jidong and Li, Yitian and Chen, Wenqing and Xiao,
                  Liqiang and He, Hao and Jin, Yaohui",
  booktitle       = "Proceedings of the 2021 Conference on Empirical Methods in
                  Natural Language Processing",
  month           = nov,
  year            = 2021,
  address         = "Online and Punta Cana, Dominican Republic",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2021.emnlp-main.303",
  doi             = "10.18653/v1/2021.emnlp-main.303",
  pages           = "3738--3747",
  abstract        = "Recently, language models (LMs) have achieved significant
                  performance on many NLU tasks, which has spurred widespread
                  interest for their possible applications in the scientific and
                  social area. However, LMs have faced much criticism of whether
                  they are truly capable of reasoning in NLU. In this work, we
                  propose a diagnostic method for first-order logic (FOL)
                  reasoning with a new proposed benchmark, LogicNLI. LogicNLI is
                  an NLI-style dataset that effectively disentangles the target
                  FOL reasoning from commonsense inference and can be used to
                  diagnose LMs from four perspectives: accuracy, robustness,
                  generalization, and interpretability. Experiments on BERT,
                  RoBERTa, and XLNet, have uncovered the weaknesses of these LMs
                  on FOL reasoning, which motivates future exploration to
                  enhance the reasoning ability.",
}

@inproceedings{webson-pavlick-2022-prompt,
  title           = "Do Prompt-Based Models Really Understand the Meaning of
                  Their Prompts?",
  author          = "Webson, Albert and Pavlick, Ellie",
  booktitle       = "Proceedings of the 2022 Conference of the North American
                  Chapter of the Association for Computational Linguistics:
                  Human Language Technologies",
  month           = jul,
  year            = 2022,
  address         = "Seattle, United States",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2022.naacl-main.167",
  pages           = "2300--2344",
  abstract        = "Recently, a boom of papers has shown extraordinary progress
                  in zero-shot and few-shot learning with various prompt-based
                  models. It is commonly argued that prompts help models to
                  learn faster in the same way that humans learn faster when
                  provided with task instructions expressed in natural language.
                  In this study, we experiment with over 30 prompts manually
                  written for natural language inference (NLI). We find that
                  models can learn just as fast with many prompts that are
                  intentionally irrelevant or even pathologically misleading as
                  they do with instructively {``}good{''} prompts. Further, such
                  patterns hold even for models as large as 175 billion
                  parameters (Brown et al., 2020) as well as the recently
                  proposed instruction-tuned models which are trained on
                  hundreds of prompts (Sanh et al., 2021). That is,
                  instruction-tuned models often produce good predictions with
                  irrelevant and misleading prompts even at zero shots. In sum,
                  notwithstanding prompt-based models{'} impressive improvement,
                  we find evidence of serious limitations that question the
                  degree to which such improvement is derived from models
                  understanding task instructions in ways analogous to humans{'}
                  use of task instructions.",
}

@article{weston2015towardsai,
  title           = {Towards ai-complete question answering: A set of
                  prerequisite toy tasks},
  author          = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and
                  Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin,
                  Armand and Mikolov, Tomas},
  journal         = {arXiv preprint arXiv:1502.05698},
  year            = 2015
}

@inproceedings{yanaka-etal-2021-sygns,
  title           = "{S}y{GNS}: A Systematic Generalization Testbed Based on
                  Natural Language Semantics",
  author          = "Yanaka, Hitomi and Mineshima, Koji and Inui, Kentaro",
  booktitle       = "Findings of the Association for Computational Linguistics:
                  ACL-IJCNLP 2021",
  month           = aug,
  year            = 2021,
  address         = "Online",
  publisher       = "Association for Computational Linguistics",
  url             = "https://aclanthology.org/2021.findings-acl.10",
  doi             = "10.18653/v1/2021.findings-acl.10",
  pages           = "103--119",
}
