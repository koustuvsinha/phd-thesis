@inproceedings{kiela-etal-2021-dynabench,
 address = {Online},
 author = {Kiela, Douwe  and
Bartolo, Max  and
Nie, Yixin  and
Kaushik, Divyansh  and
Geiger, Atticus  and
Wu, Zhengxuan  and
Vidgen, Bertie  and
Prasad, Grusha  and
Singh, Amanpreet  and
Ringshia, Pratik  and
Ma, Zhiyi  and
Thrush, Tristan  and
Riedel, Sebastian  and
Waseem, Zeerak  and
Stenetorp, Pontus  and
Jia, Robin  and
Bansal, Mohit  and
Potts, Christopher  and
Williams, Adina},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.324},
 pages = {4110--4124},
 publisher = {Association for Computational Linguistics},
 title = {Dynabench: Rethinking Benchmarking in {NLP}},
 url = {https://aclanthology.org/2021.naacl-main.324},
 year = {2021}
}

@inproceedings{luo-etal-2021-positional,
 address = {Online},
 author = {Luo, Ziyang  and
Kulmizev, Artur  and
Mao, Xiaoxi},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.413},
 pages = {5312--5327},
 publisher = {Association for Computational Linguistics},
 title = {Positional Artefacts Propagate Through Masked Language Model Embeddings},
 url = {https://aclanthology.org/2021.acl-long.413},
 year = {2021}
}

@inproceedings{wang-chen-2020-position,
 address = {Online},
 author = {Wang, Yu-An  and
Chen, Yun-Nung},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.555},
 pages = {6840--6849},
 publisher = {Association for Computational Linguistics},
 title = {What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding},
 url = {https://aclanthology.org/2020.emnlp-main.555},
 year = {2020}
}

@article{haviv2022,
 author = {Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
 journal = {ArXiv preprint},
 title = {Transformer {{Language Models}} without {{Positional Encodings Still Learn Positional Information}}},
 url = {https://arxiv.org/abs/2203.16634},
 volume = {abs/2203.16634},
 year = {2022}
}

@inproceedings{ke2021,
 author = {Guolin Ke and
Di He and
Tie{-}Yan Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KeHL21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Rethinking Positional Encoding in Language Pre-training},
 url = {https://openreview.net/forum?id=09-528y2Fgf},
 year = {2021}
}

@inproceedings{kiyono2021,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Kiyono, Shun  and
Kobayashi, Sosuke  and
Suzuki, Jun  and
Inui, Kentaro},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.266},
 pages = {3309--3321},
 publisher = {Association for Computational Linguistics},
 title = {{SHAPE}: {S}hifted Absolute Position Embedding for Transformers},
 url = {https://aclanthology.org/2021.emnlp-main.266},
 year = {2021}
}

@inproceedings{newman2020,
 address = {Online},
 author = {Newman, Benjamin  and
Hewitt, John  and
Liang, Percy  and
Manning, Christopher D.},
 booktitle = {Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
 doi = {10.18653/v1/2020.blackboxnlp-1.26},
 pages = {276--291},
 publisher = {Association for Computational Linguistics},
 title = {The {EOS} Decision and Length Extrapolation},
 url = {https://aclanthology.org/2020.blackboxnlp-1.26},
 year = {2020}
}

@inproceedings{ravishankar2021,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Ravishankar, Vinit  and
S{\o}gaard, Anders},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.59},
 pages = {763--777},
 publisher = {Association for Computational Linguistics},
 title = {The Impact of Positional Encodings on Multilingual Compression},
 url = {https://aclanthology.org/2021.emnlp-main.59},
 year = {2021}
}

@article{sanh2022,
 author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
 journal = {ArXiv preprint},
 title = {Multitask {{Prompted Training Enables Zero-Shot Task Generalization}}},
 url = {https://arxiv.org/abs/2110.08207},
 volume = {abs/2110.08207},
 year = {2021}
}

@inproceedings{vaswani2017,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{misra2020exploring,
 address = {Online},
 author = {Misra, Kanishka  and
Ettinger, Allyson  and
Rayz, Julia},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.415},
 pages = {4625--4635},
 publisher = {Association for Computational Linguistics},
 title = {Exploring {BERT}{'}s Sensitivity to Lexical Cues using Tests from Semantic Priming},
 url = {https://aclanthology.org/2020.findings-emnlp.415},
 year = {2020}
}

@inproceedings{kassner-schutze-2020-negated,
 address = {Online},
 author = {Kassner, Nora  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.698},
 pages = {7811--7818},
 publisher = {Association for Computational Linguistics},
 title = {Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly},
 url = {https://aclanthology.org/2020.acl-main.698},
 year = {2020}
}

@inproceedings{wang2021on,
 author = {Benyou Wang and
Lifeng Shang and
Christina Lioma and
Xin Jiang and
Hao Yang and
Qun Liu and
Jakob Grue Simonsen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WangSLJYLS21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 01 Sep 2021 01:00:00 +0200},
 title = {On Position Embeddings in {BERT}},
 url = {https://openreview.net/forum?id=onxoVA9FxMw},
 year = {2021}
}

@article{Liu2019:RoBERTa,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Lewis2020:BART,
 address = {Online},
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.703},
 pages = {7871--7880},
 publisher = {Association for Computational Linguistics},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 url = {https://aclanthology.org/2020.acl-main.703},
 year = {2020}
}

@inproceedings{Radford2019:GPT2,
 author = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
 title = {Language Models are Unsupervised Multitask Learners},
 year = {2019},
 url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{Zhang2022:OPT,
 author = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
 journal = {ArXiv},
 title = {OPT: Open Pre-trained Transformer Language Models},
 volume = {abs/2205.01068},
 year = {2022},
 doi = {10.48550/ARXIV.2205.01068},
 url = {https://arxiv.org/abs/2205.01068}
}



@inproceedings{Gordon2012:COPA,
 address = {Montr{\'e}al, Canada},
 author = {Gordon, Andrew  and
Kozareva, Zornitsa  and
Roemmele, Melissa},
 booktitle = {*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)},
 pages = {394--398},
 publisher = {Association for Computational Linguistics},
 title = {{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning},
 url = {https://aclanthology.org/S12-1052},
 year = {2012}
}

@inproceedings{Wang2019:SuperGLUE,
 author = {Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {3261--3275},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
Systems},
 url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
 year = {2019}
}

@inproceedings{Bisk2020:PIQA,
 author = {Yonatan Bisk and
Rowan Zellers and
Ronan LeBras and
Jianfeng Gao and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {7432--7439},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
 title = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6239},
 year = {2020}
}

@inproceedings{Sakaguchi2020:WINOGRANDE,
 author = {Keisuke Sakaguchi and
Ronan Le Bras and
Chandra Bhagavatula and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {8732--8740},
 publisher = {{AAAI} Press},
 timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
 title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6399},
 year = {2020}
}

@article{Clark2018:ARC,
  author    = {Peter Clark and
               Isaac Cowhey and
               Oren Etzioni and
               Tushar Khot and
               Ashish Sabharwal and
               Carissa Schoenick and
               Oyvind Tafjord},
  title     = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
               Challenge},
  journal   = {CoRR},
  volume    = {abs/1803.05457},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.05457},
  eprinttype = {arXiv},
  eprint    = {1803.05457},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-05457.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Dolan2005:MRPC,
 author = {Dolan, William B.  and
Brockett, Chris},
 booktitle = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
 title = {Automatically Constructing a Corpus of Sentential Paraphrases},
 url = {https://aclanthology.org/I05-5002},
 year = {2005}
}

@article{warstadt2018:CoLA,
 address = {Cambridge, MA},
 author = {Warstadt, Alex  and
Singh, Amanpreet  and
Bowman, Samuel R.},
 doi = {10.1162/tacl_a_00290},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {625--641},
 publisher = {MIT Press},
 title = {Neural Network Acceptability Judgments},
 url = {https://aclanthology.org/Q19-1040},
 volume = {7},
 year = {2019}
}

@inproceedings{salazar2019masked,
 address = {Online},
 author = {Salazar, Julian  and
Liang, Davis  and
Nguyen, Toan Q.  and
Kirchhoff, Katrin},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.240},
 pages = {2699--2712},
 publisher = {Association for Computational Linguistics},
 title = {Masked Language Model Scoring},
 url = {https://aclanthology.org/2020.acl-main.240},
 year = {2020}
}

@inproceedings{Levesque2011:WSC,
 author = {Hector J. Levesque and Ernest Davis and L. Morgenstern},
 booktitle = {KR},
 title = {The Winograd Schema Challenge},
 year = {2011},
 url = {http://commonsensereasoning.org/2011/papers/Levesque.pdf}
}


@inproceedings{Wang2018:GLUE,
 author = {Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding},
 url = {https://openreview.net/forum?id=rJ4km2R5t7},
 year = {2019}
}

@inproceedings{Dagan2005:RTE,
    title = "The Third {PASCAL} Recognizing Textual Entailment Challenge",
    author = "Giampiccolo, Danilo  and
      Magnini, Bernardo  and
      Dagan, Ido  and
      Dolan, Bill",
    booktitle = "Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing",
    month = jun,
    year = "2007",
    address = "Prague",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-1401",
    pages = "1--9",
}


@article{Matthews1975:mcc,
 author = {Brian W. Matthews},
 journal = {Biochimica et biophysica acta},
 pages = {442-51        },
 title = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme.},
 volume = {405 2},
 year = {1975},
 doi = {https://doi.org/10.1016/0005-2795(75)90109-9},
url = {https://www.sciencedirect.com/science/article/pii/0005279575901099},
}


@inproceedings{Brown2020:GPT3,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 year = {2020}
}

@article{misra2022minicons,
 author = {Kanishka Misra},
 journal = {ArXiv preprint},
 title = {minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models},
 url = {https://arxiv.org/abs/2203.13112},
 volume = {abs/2203.13112},
 year = {2022}
}

@inproceedings{
Black2022:GPTNeoX,
title={{GPT}-NeoX-20B: An Open-Source Autoregressive Language Model},
author={Sidney Black and Stella Biderman and Eric Hallahan and Quentin Gregory Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Martin Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
booktitle={Challenges {\&} Perspectives in Creating Large Language Models},
year={2022},
url={https://openreview.net/forum?id=HL7IhzS8W5}
}

@software{lm-eval-harness,
 author = {Gao, Leo and
Tow, Jonathan and
Biderman, Stella and
Black, Sid and
DiPofi, Anthony and
Foster, Charles and
Golding, Laurence and
Hsu, Jeffrey and
McDonell, Kyle and
Muennighoff, Niklas and
Phang, Jason and
Reynolds, Laria and
Tang, Eric and
Thite, Anish and
Wang, Ben and
Wang, Kevin and
Zou, Andy},
 doi = {10.5281/zenodo.5371628},
 publisher = {Zenodo},
 title = {A framework for few-shot language model evaluation},
 url = {https://doi.org/10.5281/zenodo.5371628},
 version = {v0.0.1},
 year = {2021}
}

@inproceedings{huggingface,
 address = {Online},
 author = {Wolf, Thomas  and
Debut, Lysandre  and
Sanh, Victor  and
Chaumond, Julien  and
Delangue, Clement  and
Moi, Anthony  and
Cistac, Pierric  and
Rault, Tim  and
Louf, Remi  and
Funtowicz, Morgan  and
Davison, Joe  and
Shleifer, Sam  and
von Platen, Patrick  and
Ma, Clara  and
Jernite, Yacine  and
Plu, Julien  and
Xu, Canwen  and
Le Scao, Teven  and
Gugger, Sylvain  and
Drame, Mariama  and
Lhoest, Quentin  and
Rush, Alexander},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
 doi = {10.18653/v1/2020.emnlp-demos.6},
 pages = {38--45},
 publisher = {Association for Computational Linguistics},
 title = {Transformers: State-of-the-Art Natural Language Processing},
 url = {https://aclanthology.org/2020.emnlp-demos.6},
 year = {2020}
}

@inproceedings{Devlin2019:BERT,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@article{Raffel2020:T5,
 author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
 journal = {Journal of Machine Learning Research},
 number = {140},
 pages = {1--67},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
 url = {http://jmlr.org/papers/v21/20-074.html},
 volume = {21},
 year = {2020}
}

@inproceedings{velickovic-etal-2018-graph,
 author = {Petar Velickovic and
Guillem Cucurull and
Arantxa Casanova and
Adriana Romero and
Pietro Li{\`{o}} and
Yoshua Bengio},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/VelickovicCCRLB18.bib},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Graph Attention Networks},
 url = {https://openreview.net/forum?id=rJXMpikCZ},
 year = {2018}
}


@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{longformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05150},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fairseq,
  author    = {Mikel Artetxe and
               Shruti Bhosale and
               Naman Goyal and
               Todor Mihaylov and
               Myle Ott and
               Sam Shleifer and
               Xi Victoria Lin and
               Jingfei Du and
               Srinivasan Iyer and
               Ramakanth Pasunuru and
               Giri Anantharaman and
               Xian Li and
               Shuohui Chen and
               Halil Akin and
               Mandeep Baines and
               Louis Martin and
               Xing Zhou and
               Punit Singh Koura and
               Brian O'Horo and
               Jeff Wang and
               Luke Zettlemoyer and
               Mona T. Diab and
               Zornitsa Kozareva and
               Ves Stoyanov},
  title     = {Efficient Large Scale Language Modeling with Mixtures of Experts},
  journal   = {CoRR},
  volume    = {abs/2112.10684},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10684},
  eprinttype = {arXiv},
  eprint    = {2112.10684},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10684.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shortformer,
    title = "Shortformer: Better Language Modeling using Shorter Inputs",
    author = "Press, Ofir  and
      Smith, Noah A.  and
      Lewis, Mike",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.427",
    doi = "10.18653/v1/2021.acl-long.427",
    pages = "5493--5505",
    abstract = "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
}


@misc{palm,
  doi = {10.48550/ARXIV.2204.02311},
  
  url = {https://arxiv.org/abs/2204.02311},
  
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PaLM: Scaling Language Modeling with Pathways},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12116--12128},
  year={2021},
  url={https://openreview.net/forum?id=Gl8FHfMVTZu}
}

@inproceedings{csordas2021:devil,
    title = "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    author = "Csord{\'a}s, R{\'o}bert  and
      Irie, Kazuki  and
      Schmidhuber, Juergen",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.49",
    doi = "10.18653/v1/2021.emnlp-main.49",
    pages = "619--634",
}

@inproceedings{ontanon2022:compgen,
    title = "Making Transformers Solve Compositional Tasks",
    author = "Ontanon, Santiago  and
      Ainslie, Joshua  and
      Fisher, Zachary  and
      Cvicek, Vaclav",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.251",
    doi = "10.18653/v1/2022.acl-long.251",
    pages = "3591--3607",
}

@inproceedings{dai-2019:transformerxl,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
}

@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@inproceedings{Lake2018:SCAN,
  title={Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author={Brenden M. Lake and Marco Baroni},
  booktitle={ICML},
  year={2018},
  url = {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf}
}

@inproceedings{pcfg,
  title     = {Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)},
  author    = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {5065--5069},
  year      = {2020},
  month     = {7},
  note      = {Journal track}
  doi       = {10.24963/ijcai.2020/708},
  url       = {https://doi.org/10.24963/ijcai.2020/708},
}

@article{sinha2022pos,
 author = {Koustuv Sinha and Amirhossein Kazemnejad and Siva Reddy and Joelle Pineau and Dieuwke Hupkes and Adina Williams},
 journal = {Under review at Empirical Methods of Natural Language Processing (EMNLP)},
 title = {The curious case of positional embeddings},
 year = {2022}
}
