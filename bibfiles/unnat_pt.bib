@article{liu2019b,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2021-02-05},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Others/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf},
  journal = {arXiv:1907.11692 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{wietingkiela2019,
  author={Wieting, John and Kiela, Douwe},
title     = {No Training Required: Exploring Random Encoders for Sentence Classification},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=BkgPajAcY7},
}

@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9446--9454},
  year={2018},
  url={https://arxiv.org/pdf/1711.10925.pdf}
}
@article{frankle2020training,
  title={Training batchnorm and only batchnorm: On the expressive power of random features in cnns},
  author={Frankle, Jonathan and Schwab, David J and Morcos, Ari S},
  journal={arXiv preprint arXiv:2003.00152},
  year={2020},
  url={https://arxiv.org/abs/2003.00152}
}
@inproceedings{xie2019exploring,
  title={Exploring randomly wired neural networks for image recognition},
  author={Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1284--1293},
  year={2019},
  url={https://arxiv.org/pdf/1904.01569.pdf}
}

@article{williams2018latent,
  title={Do latent tree learning models identify meaningful structure in sentences?},
  author={Williams, Adina and Drozdov, Andrew and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={253--267},
  year={2018},
  publisher={MIT Press},
  url={https://www.aclweb.org/anthology/Q18-1019/}
}

@inproceedings{scheible2013cutting,
  title={Cutting recursive autoencoder trees},
  author={Scheible, Christian and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1301.2811},
  year={2013},
  url={https://arxiv.org/pdf/1301.2811.pdf},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  booktitle = {1st International Conference on Learning Representations ({ICLR})
               Scottsdale, Arizona, USA, May 2-4, 2013, Conference Track Proceedings},
  year      = {2013},
  }

@inproceedings{jain2019attention,
  author={Jain, Sarthak and Wallace, Byron C},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {Attention is not Explanation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {3543--3556},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
}

@article{papadimitriou2020,
  title = {Learning {{Music Helps You Read}}: {{Using Transfer}} to {{Study Linguistic Structure}} in {{Language Models}}},
  shorttitle = {Learning {{Music Helps You Read}}},
  author = {Papadimitriou, Isabel and Jurafsky, Dan},
  year = {2020},
  month = oct,
  url = {http://arxiv.org/abs/2004.14601},
  urldate = {2021-02-12},
  abstract = {We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.},
  archivePrefix = {arXiv},
  eprint = {2004.14601},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Papadimitriou and Jurafsky - 2020 - Learning Music Helps You Read Using Transfer to S.pdf},
  journal = {arXiv:2004.14601 [cs]},
  language = {en},
  primaryClass = {cs}
}



@inproceedings{sinha2020b,
    title = "{UnNatural} {L}anguage {I}nference",
    author = "Sinha, Koustuv  and
      Parthasarathi, Prasanna  and
      Pineau, Joelle  and
      Williams, Adina",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.569",
    doi = "10.18653/v1/2021.acl-long.569",
    pages = "7329--7346",
    abstract = "Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. For example, in MNLI dataset we find almost all (98.7{\%}) examples contain at least one permutation which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.",
}


@article{wolf2019,
  title = {Some Additional Experiments Extending the Tech Report ''{{Assessing BERT}}'s {{Syntactic Abilities}}'' by {{Yoav Goldberg}}},
  author = {Wolf, Thomas},
  year = {2019},
  pages = {7},
  url={https://huggingface.co/bert-syntax/extending-bert-syntax.pdf},
  language = {en}
}

@article{goldberga,
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  author = {Goldberg, Yoav},
  pages = {4},
  abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ``coloreless green ideas'' subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
  language = {en},
  year={2019},
  journal = {CoRR},
  url={https://arxiv.org/pdf/1901.05287.pdf}
}

@article{gulordava2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  month = mar,
  url = {http://arxiv.org/abs/1803.11138},
  urldate = {2021-02-21},
  abstract = {Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (``The colorless green iiddeeaass I ate with the chair sslleeeepp furiously''), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors, but they also acquire deeper grammatical competence.},
  archivePrefix = {arXiv},
  eprint = {1803.11138},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Gulordava et al. - 2018 - Colorless green recurrent networks dream hierarchi.pdf},
  journal = {arXiv:1803.11138 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{rogers2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  month = nov,
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-11-11},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  archivePrefix = {arXiv},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/Adapter_BERT/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf},
  journal = {arXiv:2002.12327 [cs]},
  language = {en},
  primaryClass = {cs}
}




@article{zhangb,
  title = {{{PAWS}}: {{Paraphrase Adversaries}} from {{Word Scrambling}}},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  pages = {11},
  abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 wellformed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. Stateof-the-art models trained on existing datasets have dismal performance on PAWS ({$<$}40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Zhang et al. - PAWS Paraphrase Adversaries from Word Scrambling.pdf},
  language = {en}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018},
  url={https://arxiv.org/pdf/1804.07461.pdf}
}


@article{pham2020,
  title = {Out of {{Order}}: {{How}} Important Is the Sequential Order of Words in a Sentence in {{Natural Language Understanding}} Tasks?},
  shorttitle = {Out of {{Order}}},
  author = {Pham, Thang M. and Bui, Trung and Mai, Long and Nguyen, Anh},
  year = {2020},
  month = dec,
  abstract = {Do state-of-the-art natural language understanding models care about word order\textemdash one of the most important characteristics of a sequence? Not always! We found 75\% to 90\% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Despite BERT embeddings are famously contextual, the contribution of each individual word to downstream tasks is almost unchanged even after the word's context is shuffled. BERT-based models are able to exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequence-pair inputs in natural language inference) to make correct decisions when tokens are arranged in random orders. Encouraging classifiers to capture word order information improves the performance on most GLUE tasks, SQuAD 2.0 and out-ofsamples. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.},
  archivePrefix = {arXiv},
  eprint = {2012.15180},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Pham et al. - 2020 - Out of Order How important is the sequential orde.pdf},
  journal = {arXiv:2012.15180 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{gupta-etal-2021-bert, 
    title={BERT \& Family Eat Word Salad: Experiments with Text Understanding}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17531}, 
    number={14}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gupta, Ashim and Kvernadze, Giorgi and Srikumar, Vivek}, year={2021}, 
    month={May}, pages={12946-12954} }



@inproceedings{pimentel2020,
  title = {Information-{{Theoretic Probing}} for {{Linguistic Structure}}},
  author = {Pimentel, Tiago and Valvoda, Josef and Maudslay, Rowan Hall and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
  year = {2020},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {4609--4622},
  publisher = {Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/2020.acl-main.420.pdf}
}


@inproceedings{perez2021,
  title = {Rissanen {{Data Analysis}}: {{Examining Dataset Characteristics}} via {{Description Length}}},
  shorttitle = {Rissanen {{Data Analysis}}},
  author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  year = {2021},
  month = mar,
  abstract = {We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.},
  archivePrefix = {arXiv},
  eprint = {2103.03872},
  eprinttype = {arxiv},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Perez et al. - 2021 - Rissanen Data Analysis Examining Dataset Characte.pdf},
  booktitle = {Proceedings of the Thirty-eighth International Conference on Machine Learning (ICML)},
  language = {en},
  primaryClass = {cs, stat}
}


@inproceedings{salazar2020a,
  title = {Masked {{Language Model Scoring}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
  year = {2020},
  pages = {2699--2712},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.240},
  abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an endto-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-theart baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https: //github.com/awslabs/mlm-scoring.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Salazar et al. - 2020 - Masked Language Model Scoring2.pdf},
  language = {en}
}

@inproceedings{jawahar2019a,
  title = {What {{Does BERT Learn}} about the {{Structure}} of {{Language}}?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  year = {2019},
  pages = {3651--3657},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1356},
  abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua2.pdf},
  language = {en}
}


@inproceedings{kataoka2021,
  title = {Pre-Training without {{Natural Images}}},
  author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  year={2020},
  url={https://openaccess.thecvf.com/content/ACCV2020/papers/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.pdf},
}


@inproceedings{shen2020a,
    title = "Reservoir Transformers",
    author = "Shen, Sheng  and
      Baevski, Alexei  and
      Morcos, Ari  and
      Keutzer, Kurt  and
      Auli, Michael  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.331",
    doi = "10.18653/v1/2021.acl-long.331",
    pages = "4294--4309",
}

@inproceedings{mikolov2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  url={https://arxiv.org/pdf/1310.4546.pdf}
}

@misc{clark2019does,
      title={What Does BERT Look At? An Analysis of BERT's Attention}, 
      author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
      year={2019},
      eprint={1906.04341},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
  url={https://www.aclweb.org/anthology/N19-4009.pdf}
}

@inproceedings{dozat2016deep,
  title={Deep biaffine attention for neural dependency parsing},
  author={Dozat, Timothy and Manning, Christopher D},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
}

@article{cola_warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press},
  url={https://arxiv.org/pdf/1805.12471.pdf}
}

@inproceedings{sst2_socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013},
  url={https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}
}

@inproceedings{mrpc_dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005},
  url={https://www.aclweb.org/anthology/I05-5002.pdf}
}

@article{stsb_cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

@article{mnli_williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017},
  url={https://www.aclweb.org/anthology/N18-1101.pdf}
}

@article{qnli_rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016},
  url={https://arxiv.org/pdf/1606.05250.pdf}
}

@article{qnli_2_demszky2018transforming,
  title={Transforming question answering datasets into natural language inference datasets},
  author={Demszky, Dorottya and Guu, Kelvin and Liang, Percy},
  journal={arXiv preprint arXiv:1809.02922},
  year={2018},
  url={https://arxiv.org/pdf/1809.02922.pdf}
}

@inproceedings{rte1_dagan2005pascal,
  title={The {PASCAL} recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2005},
  organization={Springer},
  url={https://link.springer.com/chapter/10.1007/11736790_9}
}

@inproceedings{rte2_haim2006second,
  title={The second {PASCAL} recognising textual entailment challenge},
  author={Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
  year={2006},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.8552&rep=rep1&type=pdf}
}

@inproceedings{rte3_giampiccolo2007third,
  title={The third {PASCAL} recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007},
  url={https://www.aclweb.org/anthology/W07-1401.pdf}
}

@inproceedings{rte5_bentivogli2009fifth,
  title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TAC},
  year={2009},
  url={https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.1231&rep=rep1&type=pdf}
}

@inproceedings{wnli_levesque2012winograd,
  title={The {Winograd} schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012},
  organization={Citeseer}
}

@article{bies2012english,
  title={English web treebank},
  author={Bies, Ann and Mott, Justin and Warner, Colin and Kulick, Seth},
  journal={Linguistic Data Consortium, Philadelphia, PA},
  year={2012},
  url={https://catalog.ldc.upenn.edu/LDC2012T13}
}

@inproceedings{silveira2014gold,
  title={A Gold Standard Dependency Corpus for English.},
  author={Silveira, Natalia and Dozat, Timothy and De Marneffe, Marie-Catherine and Bowman, Samuel R and Connor, Miriam and Bauer, John and Manning, Christopher D},
  booktitle={LREC},
  pages={2897--2904},
  year={2014},
  organization={Citeseer},
  url={http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhang2019paws,
  title={PAWS: Paraphrase adversaries from word scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  journal={arXiv preprint arXiv:1904.01130},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1131.pdf}
}

@inproceedings{liu-etal-2019-linguistic,
    title = "Linguistic Knowledge and Transferability of Contextual Representations",
    author = "Liu, Nelson F.  and
      Gardner, Matt  and
      Belinkov, Yonatan  and
      Peters, Matthew E.  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1112",
    doi = "10.18653/v1/N19-1112",
    pages = "1073--1094",
    abstract = "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
}

@inproceedings{alain2016understanding,
  author    = {Guillaume Alain and
               Yoshua Bengio},
  title     = {Understanding intermediate layers using linear classifier probes},
  booktitle = {{ICLR} 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HJ4-rAVtl},
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://www.aclweb.org/anthology/N19-1419.pdf}
}

@article{ettinger2020,
  title = {What {{BERT Is Not}}: {{Lessons}} from a {{New Suite}} of {{Psycholinguistic Diagnostics}} for {{Language Models}}},
  shorttitle = {What {{BERT Is Not}}},
  author = {Ettinger, Allyson},
  year = {2020},
  month = dec,
  volume = {8},
  pages = {34--48},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00298},
  abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\textemdash and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
  file = {/Users/koustuvs/Google Drive/Paperpile/Zotero/NLI Generalization/Ettinger - 2020 - What BERT Is Not Lessons from a New Suite of Psyc.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}

@article{rissanen1984universal,
  title={Universal coding, information, prediction, and estimation},
  author={Rissanen, Jorma},
  journal={IEEE Transactions on Information theory},
  volume={30},
  number={4},
  pages={629--636},
  year={1984},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/1056936}
}

@inproceedings{bansal2019learning,
  title= {Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks},
  author={Bansal, Trapit and Jha, Rishikesh and McCallum, Andrew},
  booktitle = {Proceedings of {COLING} 2020},
  editor    = {Donia Scott and
               N{\'{u}}ria Bel and
               Chengqing Zong},
  pages     = {5108--5123},
  publisher = {International Committee on Computational Linguistics},
  year      = {2020},
}


@inproceedings{zhang2021a,
  author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
  month = mar,
  abstract = {This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.},
  title     = {Revisiting Few-sample {BERT} Fine-tuning},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=cO1IH43yUF},
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391--407},
  year={1990},
  publisher={Wiley Online Library},
  url={https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-4571(199009)41:6\%3C391::AID-ASI1\%3E3.0.CO;2-9}
}

@article{landauer1997solution,
  title={A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
  author={Landauer, Thomas K and Dumais, Susan T},
  journal={Psychological review},
  volume={104},
  number={2},
  pages={211},
  year={1997},
  publisher={American Psychological Association},
  url={https://psycnet.apa.org/record/1997-03612-001}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008},
  url={https://dl.acm.org/doi/10.1145/1390156.1390177}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018},
  url={https://arxiv.org/pdf/1802.05365.pdf}
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis},
  url={https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720155/}
}

@article{greenberg1963some,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  author={Greenberg, Joseph},
  journal={In J. Greenberg, ed., Universals of Language. 73-113. Cambridge, MA.},
  year={1963},
  publisher={mit Press},
  url={https://wals.info/refdb/record/Greenberg-1963}
}

@article{levy2015improving,
  title={Improving distributional similarity with lessons learned from word embeddings},
  author={Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={211--225},
  year={2015},
  publisher={MIT Press},
  url={https://www.aclweb.org/anthology/Q15-1016.pdf}
}


@article{Hahn2347,
  title = {Universals of Word Order Reflect Optimization of Grammars for Efficient Communication},
  author = {Hahn, Michael and Jurafsky, Dan and Futrell, Richard},
  year = {2020},
  volume = {117},
  pages = {2347--2353},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  doi = {10.1073/pnas.1910923117},
  abstract = {Human languages share many grammatical properties. We show that some of these properties can be explained by the need for languages to offer efficient communication between humans given our cognitive constraints. Grammars of languages seem to find a balance between two communicative pressures: to be simple enough to allow the speaker to easily produce sentences, but complex enough to be unambiguous to the hearer, and this balance explains well-known word-order generalizations across our sample of 51 varied languages. Our results offer quantitative and computational evidence that language structure is dynamically shaped by communicative and cognitive pressures.The universal properties of human languages have been the subject of intense study across the language sciences. We report computational and corpus evidence for the hypothesis that a prominent subset of these universal properties\textemdash those related to word order\textemdash result from a process of optimization for efficient communication among humans, trading off the need to reduce complexity with the need to reduce ambiguity. We formalize these two pressures with information-theoretic and neural-network models of complexity and ambiguity and simulate grammars with optimized word-order parameters on large-scale data from 51 languages. Evolution of grammars toward efficiency results in word-order patterns that predict a large subset of the major word-order correlations across languages.},
  eprint = {https://www.pnas.org/content/117/5/2347.full.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  number = {5}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015},
  url={https://arxiv.org/pdf/1506.06724.pdf}
}

@article{dryer1992greenbergian,
  title={The {G}reenbergian word order correlations},
  author={Dryer, Matthew S},
  journal={Language},
  pages={81--138},
  year={1992},
  publisher={JSTOR}
}

@book{cinque1999adverbs,
  title={Adverbs and functional heads: A cross-linguistic perspective},
  author={Cinque, Guglielmo},
  year={1999},
  publisher={Oxford University Press on Demand}
}

@article{belinkov2021probing,
  title={Probing Classifiers: Promises, Shortcomings, and Alternatives},
  author={Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2102.12452},
  year={2021}
}

@article{hupkes2018visualisation,
  title={Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}

@inproceedings{torroba-hennigen-etal-2020-intrinsic,
    title = "Intrinsic Probing through Dimension Selection",
    author = "Torroba Hennigen, Lucas  and
      Williams, Adina  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.15",
    doi = "10.18653/v1/2020.emnlp-main.15",
    pages = "197--216",
    abstract = "Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.",
}

@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = {2020},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@book{chomsky1957syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  year={1957},
  publisher={Walter de Gruyter}
}

@article{bowman2021will,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Bowman, Samuel R and Dahl, George E},
  journal={arXiv preprint arXiv:2104.02145},
  year={2021}
}

@article{lakretz2021mechanisms,
title = {Mechanisms for handling nested dependencies in neural-network language models and humans},
journal = {Cognition},
pages = {104699},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104699},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721001189},
author = {Yair Lakretz and Dieuwke Hupkes and Alessandra Vergallito and Marco Marelli and Marco Baroni and Stanislas Dehaene},
}

@inproceedings{papadimitriou-etal-2021-deep,
    title = "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual {BERT}",
    author = "Papadimitriou, Isabel  and
      Chi, Ethan A.  and
      Futrell, Richard  and
      Mahowald, Kyle",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.215",
    pages = "2522--2532",
    abstract = "We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a {``}subject{''}) is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.",
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{yu2021interplay,
  title={On the Interplay Between Fine-tuning and Composition in Transformers},
  author={Yu, Lang and Ettinger, Allyson},
  journal={arXiv preprint arXiv:2105.14668},
  year={2021}
}

@article{mccoy2019,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  month = jun,
  journal = {arXiv:1902.01007 [cs]},
  eprint = {1902.01007},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1902.01007},
  urldate = {2020-11-10},
  abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
  archiveprefix = {arXiv},
  language = {en},
}

@inproceedings{parthasarathi2021want,
      title={Sometimes We Want Ungrammatical Translations},
      author={Prasanna Parthasarathi and Koustuv Sinha and Joelle Pineau and Adina Williams},
      abstract={Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case. },
      year={2021},
      arxiv={2104.07623},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      booktitle={Findings of the Association for Computational Linguistics:  Empirical Methods in Natural Language Processing (EMNLP)},
      url={https://arxiv.org/pdf/2104.07623},
}

@article{ettinger-2020-whatbertisnot,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
}
@inproceedings{alleman-etal-2021-syntactic,
    title = "Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models",
    author = "Alleman, Matteo  and
      Mamou, Jonathan  and
      A Del Rio, Miguel  and
      Tang, Hanlin  and
      Kim, Yoon  and
      Chung, SueYeon",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.27",
    doi = "10.18653/v1/2021.repl4nlp-1.27",
    pages = "263--276",
}

@article{barzilay2008modeling,
  title={Modeling local coherence: An entity-based approach},
  author={Barzilay, Regina and Lapata, Mirella},
  journal={Computational Linguistics},
  volume={34},
  number={1},
  pages={1--34},
  year={2008},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{yu-ettinger-2021-interplay,
    title = "On the Interplay Between Fine-tuning and Composition in Transformers",
    author = "Yu, Lang  and
      Ettinger, Allyson",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.201",
    doi = "10.18653/v1/2021.findings-acl.201",
    pages = "2279--2293",
}

@inproceedings{laban-etal-2021-transformer,
    title = "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test",
    author = "Laban, Philippe  and
      Dai, Luke  and
      Bandarkar, Lucas  and
      Hearst, Marti A.",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.134",
    doi = "10.18653/v1/2021.acl-short.134",
    pages = "1058--1064",
}

@inproceedings{glavas-vulic-2021-supervised,
    title = "Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation",
    author = "Glava{\v{s}}, Goran  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.270",
    pages = "3090--3104",
}

@article{oconnor2021context,
      title={What Context Features Can Transformer Language Models Use?}, 
      author={Joe O'Connor and Jacob Andreas},
      year={2021},
      eprint={2106.08367},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
